% Chapter 5

\chapter{Introduction} % Write in your own chapter title
\label{Chapter5}
\lhead{Chapter 5. \emph{Introduction}} % Multi-label classification


\section{Overview}

In machine learning literature, multi-label classification is a variant of the classification problem in which each instance has several labels. In general, the task of multi-label learning is to find a model that can maps inputs \textbf{x} to binary vector \textbf{y}. While it is different with multi-class classfication in terms of the later one scalars outputs in classification problem.

\graphicspath{ {./Figures/} }
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.8\textwidth]{beach.jpg}
    \caption{Example Image}%
    \label{fig:MultilableImage}%
\end{figure}

The difference between single-label classification and multi-label classification is the outputs of datasets.
In Figure \ref{fig:MultilableImage}, we can classify it as a picture of a beach in classification literature $\in \{Yes, No\}$. In multi-label literature, we can tag sea, beach, chairs, sky, cloud for the picture $\in \{beach, sea, chairs, sand\}$.

\begin{table}
\centering
\begin{tabular}{|c c c c c | c c c c|}
\hline
$X_{1}$ & $X_{2}$ &  $X_{3}$ & $X_{4}$ & $X_{5}$ & $Y_{1}$ & $Y_{2}$ & $Y_{3}$ & $Y_{4}$ \\
\hline
1 & 0.4 & 0.2 & 0 & 1 & 0 & 1 & 1 & 0 \\
0 & 0.2 & 0.6 & 1 & 1 & 1 & 0 & 1 & 0 \\
0 & 0.5 & 0.8 & 1 & 1 & 0 & 0 & 1 & 0 \\
1 & 0.1 & 0.4 & 0 & 1 & 1 & 1 & 1 & 0 \\
1 & 0.8 & 0.2 & 0 & 1 & 0 & 1 & 1 & 0 \\
\hline
0 & 0.5 & 0.3 & 1 & 1 & ? & ? & ? & ? \\
\hline
\end{tabular}
\caption{Multilabel $Y_{1},...,Y_{L} \in 2^L$}
\ref{tab:MultilabelTable}
\end{table}

In general, there exists two main approaches of tackling the multi-label classification problem. One transfers a multi-label problem into a set of binary classification problems which can be handled by a set of binary classifiers. The other one applies algorithms to complete multi-label classification. 

Several problem transformation methods can be applied to multi-label classification. A baseline method, named the binary relevance method\citep{read2011classifier}, trains one binary classifier for each label independently. Depending on the results of the classifiers, the combined model predicts all labels for an unseen sample. The method divides the problem into multiple binary works which has something in common with the one-vs-all method for multi-class classification. Problem transformation benefits from scalability and flexibility because single-label classifier can be implemented to job. Support Vector Machine, Naive Bayes, $k$ Nearest Neighbor have been used in the method\citep{read2011classifier}.

There are some other transformation methods, for instance label powerset transformation. The method builds up one binary classifier for each label combination verified in the training dataset\citep{tsoumakas2006multi}. The random $k$-labelsets algorithm\citep{tsoumakas2007random} utilizes multiple label powerset classifier which is trained on a random sub dataset of the labels. A voting scheme is used to predict unseen samples via a ensemble method.

Multilayer neural network can be trained to learn a model from multi-label examples. We will generate a 3 labels dataset and use a 2 layer network to do classification.

\section{Multi-Label Learning}
Let $X = R^d$ represent the domain of dataset and let $Y = {1,2,..,L}$ be the finite set of labels. Given that we have a set of training dataset $T = {(x_{1},Y_{1}),(x_{2},Y_{2}),...,(x_{l},Y_{l})} (x_{i} \in X, Y_{i} \subset Y)$ which are extracted from an unknown distribution $D$. Our target of task is to learn a multi-label classifier $h: X \to 2^y $ via optimizing specific evaluation metric. However, instead of learning a multi-label classifier, we will learn a function $f$ while $f(X) \to R^d$. Supposing that a high performance classifier can output a closer subset for labels in $Y_{i}$ than those missing or exceeding in $Y_{i}$, which means $f(x_{i}, y_{1}) > f(x_{i}, y_{2})$ if $y_{1} \in Y{i}$ and $y_{2} \notin Y{i}$. We can transfer real valued function $f(\cdot , \cdot)$ to a ranking function $r(\cdot , \cdot)$ that maps the outputs of $f(x_{i}, y_{1})$ to any $y \in Y$ if $f(x_{i}, y_{1}) > f(x_{i}, y_{2})$. It is worth noting that the multi-label classifier $h(\cdot)$ can be derived from the function $f(\cdot , \cdot)$ where $h(x_{i}) = {y|f(x_{i}, y) > t(x_{i}), y \in Y}$, and $t(\cdot)$ is a threshold function.

Single-label and multi-class classification can be regarded as two degenerated variants of multi-label learning problem if each sample has only one single label. However, multi-label problem is much more difficult than traditional single-label problems because of high dimensional output space. For example, the number of label sets increases exponentially with increasing number of class labels. If there are $10$ class labels for dataset, there are $2^20$ possible label sets maximun.

The challenge of huge combination of output labels is hard to overcome. One of methods is to investigate dependency among labels. For example, if an image labelled with $castle$, it would be highly labelled with $brick$ and $mountain$. A movie which is labelled with $comedy$ is unlikely related to a $documentary$. Therefore, successful exploitation of the label correlations is regarded as an effective approach to high accuracy multi-label learning process. There are three categories, in terms of order of correlations, for existing strategies to find the relation between labels. They are first-order strategy, second-order strategy and high-order strategy.

\section{Loss Function}

To adapt a neural network handling single-label instances to multi-label instances, we need to design a specific loss function instead of squared error loss function to track the characteristics of multi-label learning and some revision of stochastic gradient descent. 

\section{Artificial Neural Networks}



\section{Weather Classification}




