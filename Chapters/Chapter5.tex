% Chapter 5

\chapter{Introduction} % Write in your own chapter title
\label{Chapter5}
\lhead{Chapter 5. \emph{Introduction}} % Multi-label classification

\section{Overview}

In the literature of machine learning, multi-label classification is a variant of classification problems where each sample has several labels. In general, the task of multi-label learning is to train a model that can map inputs \textbf{x} to a set of binary vectors \textbf{y}. It differs with multi-class classfication in terms of the output label space.

\graphicspath{ {./Figures/} }
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.8\textwidth]{beach.jpg}
    \caption{Example Image}%
    \label{fig:MultilableImage}%
\end{figure}

The difference between single-label classification and multi-label classification is the number of output labels. In Figure \ref{fig:MultilableImage}, we can classify it as a picture of a beach in single-label classification problem $\in \{Yes, No\}$. In multi-label classification, we can tag beach, sea, chairs, sand for the picture $\in \{beach, sea, chairs, sand\}$.

\begin{table}
\centering
\begin{tabular}{|c c c c c | c c c c|}
\hline
$X_{1}$ & $X_{2}$ &  $X_{3}$ & $X_{4}$ & $X_{5}$ & $Y_{1}$ & $Y_{2}$ & $Y_{3}$ & $Y_{4}$ \\
\hline
1 & 0.4 & 0.2 & 0 & 1 & 0 & 1 & 1 & 0 \\
0 & 0.2 & 0.6 & 1 & 1 & 1 & 0 & 1 & 0 \\
0 & 0.5 & 0.8 & 1 & 1 & 0 & 0 & 1 & 0 \\
1 & 0.1 & 0.4 & 0 & 1 & 1 & 1 & 1 & 0 \\
1 & 0.8 & 0.2 & 0 & 1 & 0 & 1 & 1 & 0 \\
\hline
0 & 0.5 & 0.3 & 1 & 1 & ? & ? & ? & ? \\
\hline
\end{tabular}
\caption{Multilabel $Y_{1},...,Y_{L} \in 2^L$}
\label{tab:MultilabelTable}
\end{table}

In general, there are two main approaches to tackle the multi-label classification problem. One method is to transfer a multi-label problem into a set of binary classification problems which can be handled by a set of binary classifiers. The other one applies algorithms to classify multi-label images directly. 

Several problem transformation methods can be applied to multi-label classification. A baseline method, the binary relevance method \citep{read2011classifier}, trains one binary classifier for each label independently. Depending on the results of the classifiers, the combined model predicts all labels for a test sample. The method divides the problem into multiple binary works which are common in something with the one-vs-all method for multi-class classification. Problem transformation methods benefit from scalability and flexibility because single-label classifier can be implemented easily. SVM, Naive Bayes, and $k$ Nearest Neighbor have been used in the method \citep{read2011classifier}.

Other transformation methods include label powerset transformation. The method builds up one binary classifier for each label combination verified in the training dataset \citep{tsoumakas2006multi}. The random $k$-labelsets algorithm \citep{tsoumakas2007random} utilises a multi-label powerset classifier which is trained on a random subset of the labels. Finally, a voting scheme predicts test samples via an ensemble method.

\section{Multi-Label Learning}
Let $X = R^d$ represent the domain of dataset and let $Y = {1,2,..,L}$ be the finite set of labels. Given that we have a set of training dataset $T = {(x_{1},Y_{1}),(x_{2},Y_{2}),...,(x_{l},Y_{l})} (x_{i} \in X, Y_{i} \subset Y)$ which are extracted from an unknown distribution $D$. Target of task is to learn a multi-label classifier $h: X \to 2^y $ via optimising specific evaluation metric. However, instead of learning a multi-label classifier, we will learn a function $f$ while $f(X) \to R^d$. Given that a high performance classifier can output a closer subset for labels in $Y_{i}$ than those missing or exceeding in $Y_{i}$, then $f(x_{i}, y_{1}) > f(x_{i}, y_{2})$ if $y_{1} \in Y{i}$ and $y_{2} \notin Y{i}$. We can transfer real valued function $f(\cdot , \cdot)$ to a ranking function $r(\cdot , \cdot)$ that maps the outputs of $f(x_{i}, y_{1})$ to any $y \in Y$ if $f(x_{i}, y_{1}) > f(x_{i}, y_{2})$. It is worth noting that the multi-label classifier $h(\cdot)$ can be derived from the function $f(\cdot , \cdot)$, where $h(x_{i}) = {y|f(x_{i}, y) > t(x_{i}), y \in Y}$, and $t(\cdot)$ is a threshold function.

Single-label and multi-class classification can be regarded as two degenerated variants of multi-label learning problem if each sample has only one single label. However, multi-label problem is much more difficult than traditional single-label problems because of high dimensional output space. For example, the number of label sets increases exponentially with increasing number of class labels. If there are $10$ class labels for dataset, there are $2^{10}$ possible label sets maximun.

A huge combination of output labels is a challenge. One of methods is to investigate dependency among labels to reduce label space. For example, if an image labelled with $castle$, it would be highly labelled with $brick$ and $mountain$. A movie, is labelled with $comedy$, is unlikely to be related to a $documentary$. Therefore, successful exploitation of label correlations is regarded as an effective approach to high accuracy multi-label learning machine. There are three strategies, depending on the order of the label correlation, to find the relation between labels. They are first order strategy, second order strategy and high order strategy. 

The first order strategy treats the label by label independently and ignores correlation between labels. It can be regarded as decomposing a multi-label learning problem into a set of binary classification problems based on each label. The method benefits from simple computation and high efficiency. However, accuracy could be suboptimal because of ignoring the correlation of labels.

The second order strategy considers pairwise relations between labels. For example, the interaction between any pair of labels, or the ranking between related and unrelated labels. The method achieves good generalisation performance because the label correlations are investigated in some extent. In real world, there are higher order correlations than second order assumption in many applications.

The high order strategy investigates more than $2$ order correlations among labels which can be the influence on each label or addressing connections among sub space of output labels. It is obvious that the high order strategy is more capable than the previous two strategies on the cost of complexity and intensive computation.
