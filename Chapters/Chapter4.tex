% Chapter 4

\chapter{Experiment} % Write in your own chapter title
\label{Chapter4}
\lhead{Chapter 4. \emph{Experiment}}

The experiment environment includes hardware, which is i7 CPU, 8G RAM and a GeForce GTX 770, and software, which is Ubuntu Linux 14.04 and Caffe\citep{jia2014caffe}, a deep learning software framework. We train the model in Caffe with implemented spatial pyramid pooling layers.

\section{Training Neural Network}

We trained model with the 1000-category ImageNet2012 dataset. We use the network architecture of the model~\cite{ZeilerF13} which achieved an excellent result in the 2013 ImageNet Competition. We implement SPP with CUDA C language and deploy it before the first fully-connected layer.

In the experiment, a subset of ImageNet dataset, which contains 1.2 million labelled high-resolution images depicting 1000 object categories for training and 50,000 validation images, is used as training dataset. Most images of the dataset are multi-scale and some are grey.To feed them into the network conveniently, it is better to convert them with a constant dimensionality. The images are resized to $256\times256$. And for grey images, they are combined triple times to simulate RGB images. The model is trained on raw RGB values of pixels. Activation function is the Rectified Linear Units(ReLUs), which ensures fast training of the neural network.

\begin{equation}\label{eq:ReLU}
f(x) = max(0, x)
\end{equation}

The model \ref{fig:ImageNetArch} is trained with a descent optimisation method, stochastic gradient descent. The batch size is 128, and the momentum is 0.9.  In each layer, the weights are initialised with a Gaussian distribution which has  a $0$ mean, and $0.01$ standard deviation. The neuron biases, in the $Conv_{2}$, $Conv_{4}$, $Conv_{5}$ and fully connected layers, are initialized with constant 1, while other biases are initialised with constant 0.

The learning rates are set equally for all layers. They are initialised at 0.01 and decrease with the stepdown policy which means that they would drop by a factor of 10 after each 100000 iteration. In total, the learning rate drops 3 times and the accuracy keeps stable after 370K iterations. 

The training is regularised via techniques, dropout and weight decay. The dropout regularisation is implemented at the two fully-connected layers and the dropout ratio is set to 0.5. The neurons, which are dropped out, output zero and do not participate in the backpropagation process. Therefore, the neural network samples different architectures each time. It, significantly, decreases complex co-adaptations of neurons because a neuron does not depend on the existence of all other neurons. The weight decay, $\epsilon$, is set to 0.0005 which means the new weight is shrunk according to 
\begin{equation}\label{eq:ffEq}
w^{new} = w^{old}(1 - \epsilon)
\end{equation}
after each weight update.

\section{Fine-tuning Model}

There are two challenges. First, the CNN model is trained to classify objects in images. However, the current task is to classify weather scenes. This can be solved by changing the output of the model from $1000$ to $2$. Second, the CNN architecture contains more than 60 million parameters which are too many for the weather classification dataset.The target dataset has only 10000 images in total. It is insufficient and the model is easy to overfit. It can be solved by training on the pre-trained CNN model. Because the learned model is close to optimism, it needs tiny adjustment only.

The fine-tuning transfers weights of each layers from the previous model to the new model except for the last fully connected layer. The last layer is taken over by a new layer which contains the same amount of neurons equally to the class number, say two. The weights in the replaced layer are initialized with random values. One advantage of fine-tune is keeping minimum risk of overfitting. The other one is that weights reach quickly optimal values.

In the task, we want to classify sunny and cloudy images. The last layer of the network, $FC8$, is taken place by a new layer with two neurons according to two types of weather conditions. The weights of the pre-trained model are used to initialise the weights in the new neural network. 

To minimise the risk of overfitting, data augmentation is utilised in the experiment. New patches are generated from each image via transformations, say flipping and cropping. The job is done by increasing the spatial generalisation capability of the model. Five patches are cropped from four corners and center of the image, and then they are flipped. In total, ten new images are generated with the same label.

$1000$ images are kept out to evaluate the model. One iteration is a process of feeding a batch of images into the network. There are $10000$ iterations in the full training process, which means that total training image number is $128\times10000$. One epoch is that the total training images are fed into the network once. Dividing the size of training images , $9000$, the number of epochs is $142$. The initial base learning rate is $0.001$ and the rate is divided by $10$ every $10$ epochs. Because the weights in $FC8$ are randomly initialised which means they are not close to final optimization value, the learning rate of $FC8$ is $10$ times of the base learning rate in order to converge quickly.

\section{Companion Experimental}

To compare the performance of fine-tuned model, we do some a experiment with other method. We use a pre-trained model with AlexNet architecture and extract features from fc7. Then we apply SVM on the features and learn a classifier to do weather classification.

\section{Experimental Result}

The results in table\ref{ExpRes} shows that supervised pre-training model have excellent generalization capability. 

\begin{table}[h]
\begin{center}
    \begin{tabular}{| c | c | c | c | c |}
    \hline
    Methods & CNN+SVM & SPP+SVM & Finetune on CNN & Finetune on SPP  \\ \hline
    Accuracy & $84.8\%$ & $82.1\%$ & $93.1\%$ & $93.98\%$ \\ \hline
    \end{tabular}
    \caption{CNN means with original trained model, SPP means model deployed with spatial pyramid pooling layer}
    \label{ExpRes}
\end{center}
\end{table}

The fine-tuning process is very quick, and accuracy rate convergences to over $90\%$. 
After 12 epoch, the accuracy rate exceeds $90\%$. 
\graphicspath{ {./Figures/} }
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.8\textwidth]{FinetuneAccuracy.png}
    \caption{Finetune Process}%
    \label{fig:finetuneprocess}%
\end{figure}

No single image dataset can totally capture variation in natural images, so even ImageNet is biased in some fields. Then pre-training may be make the CNN to overfit and then damage model generalization capability. To learning the process of ImageNet pre-training, we look into the effect of pre-training period on generalization performance with and without fine-tuning. In figure \ref{fig:finetuneprocess}, we can find that longer pre-training improves performance.

To have a more detailed information of fine-tuning process, we can have a closer look at training loss. We plot the curve of first 200 iterations and loss value.

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.4\textwidth]{finetuneCNNProcess.png}
	\includegraphics[width=0.4\textwidth]{finetuneSPPProcess.png}
    \caption{Finetune Process}%
    \label{fig:FTvsSC}%
\end{figure}

From figure \ref{fig:FTvsSC}, we can find that the fine-tuning procedure produces a more smooth loss function curve and ceases with a better loss. In the left figure, the loss value of fine tune procedure is higher than no fine tune process at the beginning, then it shrinks sharply and keeps lower. In the other figure, we can find that starting loss value are both less than in the left figure. And the loss values of fine tune are less than no fine tune process. We can conclude that generally fine tune is a effective approach to reduce loss value and fine tune on SPP model can achieve a better result than CNN model.

\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.8\textwidth]{ROCWeatherClassification.png}
    \caption{ROC Curve}%
    \label{fig:WeatherClassificationROC}%
\end{figure}



\section{Architecture Analysis}

Although CNNs have excellent performance in computer vision field, there is still a long way to understand insight working mechanism. In order to have more insight about the reason why deep layers can increase performance of visual sentiment classification, we will do some analysis on fine-tuned network.

The outputs of each layers have been treated as visual descriptors \citep{razavian2014cnn}, while the vectors are composed by outputs of neurons in previous layer. While the lower layers encode low-level features, top layers have been used to capture high-level information. In other words, we can train classifiers basing on features extracted from each layer.

In fully-connected layer, units can be represented as d dimensional vectors, and no further manipulation is needed. The layer takes all outputs of previous layer ,for example CONV, POOL, NORM,whose feature maps are multidimensional, and flats the feature maps into d dimensional vectors. The outputs of fully connected layer are feed into a classifier. Two types of linear classifiers are used. They are linear Support Vector Machine and Softmax.

We have a look for the feature maps of each convolutional layers a cloudy image.

\graphicspath{ {./Figures/DifferentLayers/} }

\begin{figure}[htb]
    \centering
    \subfigure[raw image]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{cloudy1}
		   		\label{fig:cloudy}
		\end{minipage}
    	} 
    \subfigure[conv1 output]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{cloudy1_conv1_fm}
		   		\label{fig:cloudy_conv1}
		\end{minipage}
    	} 
    \subfigure[conv2 output]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{cloudy1_conv2_fm}
		   		\label{fig:cloudy_conv2}
		\end{minipage}
    	} 
    \subfigure[conv3 output]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{cloudy1_conv3_fm}
		   		\label{fig:cloudy_conv3}
		\end{minipage}
    	} 
    \subfigure[conv4 output]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{cloudy1_conv4_fm}
		   		\label{fig:cloudy_conv4}
		\end{minipage}
    	} 
    \subfigure[conv5 output]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{cloudy1_conv5_fm}
		   		\label{fig:cloudy_conv5}
		\end{minipage}
    	} 

    \caption{A cloudy image and outputs of convolutional layers}%

    \label{fig:cloudy_finetuneprocess}%
\end{figure}

In Figure \ref{fig:cloudy_finetuneprocess}, a cloudy image is fed into CNN and b-d are outputs of different convolutional layers. For the sake of image size, there are only parts of filters are plotted in images b-d.

Compared to the previous cloudy image, we can have a look for similar outputs of a sunny image.

\begin{figure}[!htb]
    \centering
    \subfigure[raw image]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{sunny2}
		   		\label{fig:sunny}
		\end{minipage}
    	} 
    \subfigure[conv1 output]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{sunny2_conv1_fm}
		   		\label{fig:sunny_conv1}
		\end{minipage}
    	} 
    \subfigure[conv2 output]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{sunny2_conv2_fm}
		   		\label{fig:sunny_conv2}
		\end{minipage}
    	} 
    \subfigure[conv3 output]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{sunny2_conv3_fm}
		   		\label{fig:sunny_conv3}
		\end{minipage}
    	} 
    \subfigure[conv4 output]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{sunny2_conv4_fm}
		   		\label{fig:sunny_conv4}
		\end{minipage}
    	} 
    \subfigure[conv5 output]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{sunny2_conv5_fm}
		   		\label{fig:sunny_conv5}
		\end{minipage}
    	} 

    \caption{A sunny image and outputs of convolutional layers}%

    \label{fig:sunny_finetuneprocess}%
\end{figure}

The convolutional layers output the same number filters as Figure \ref{fig:cloudy_finetuneprocess}. Neural activations in fully connected layers can act as d-dimensional vectors, where d is the number of neurons. Not like the earlier layers, for instance CONV and POOLING whose feature maps are multimensional. In figure \ref{fig:ImageNetArch}, we can get that the feature maps of CONV1 are $96\times55\times55$ dimension and the feature maps of CONV5 are $256\times13\times13$ dimension. The feature maps of CONV5 are downsampled by POOL5 and flatten into d-dimensional vectors and used as features for classifier. We can use two types of classifiers. One is linear support vector machine and the other is SoftMax.

\section{Transfer Learning}

Because we do not have sufficient size of dataset, comparing to ImangNet, we use a pretrained model based on ImageNet dataset and then use the CNN either as a fixed feature extractor or an initialization for our task. After fine tuning, weights have been updated for fitting new functions. Accordingly, the feature maps have some difference.

\begin{figure*}[!htb]
\begin{center}

\begin{tabular}{|c|c|c|c|c|c|} \hline\hline
Image & Conv1 & Conv2 & Conv3 & Conv4 & Conv5  \\ \hline
\includegraphics[scale=0.1]{sunny2.png} &
\includegraphics[scale=0.1]{sunny2_caffe_conv1_fm.png} &
\includegraphics[scale=0.1]{sunny2_caffe_conv2_fm.png} &
\includegraphics[scale=0.1]{sunny2_caffe_conv3_fm.png} &
\includegraphics[scale=0.1]{sunny2_caffe_conv4_fm.png} &
\includegraphics[scale=0.1]{sunny2_caffe_conv5_fm.png}\\ \hline\hline

Image & Conv1 & Conv2 & Conv3 & Conv4 & Conv5  \\ \hline
\includegraphics[scale=0.1]{sunny2.png} &
\includegraphics[scale=0.1]{sunny2_conv1_fm.png} &
\includegraphics[scale=0.1]{sunny2_conv2_fm.png} &
\includegraphics[scale=0.1]{sunny2_conv3_fm.png} &
\includegraphics[scale=0.1]{sunny2_conv4_fm.png} &
\includegraphics[scale=0.1]{sunny2_conv5_fm.png}\\ \hline\hline
\end{tabular}

\end{center}
	\caption{Visiualization of feature maps outputs between original CNN model and CNN-SPP model. The upper images are from original model and the lower images are from fine tuned model}
	\label{fig:diff_featuremap}%
\end{figure*}

\begin{figure}[htb]
    \centering
	\includegraphics[width=0.4\textwidth]{sunny2_hist_caffe_fc7.png}
	\includegraphics[width=0.4\textwidth]{sunny2_hist_spp_fc7.png}
    \caption{Histogram distribution of vectors from FC7. Left one is from CNN model and right one is from finetuned model}%
    \label{fig:fc7_hist_output}%
\end{figure}

From the feature maps and histogram distribution maps, We can find that the outputs are slightly different. The fine tuned model generates smoother histogram distribution. 

\section{Error Results}

The images \ref{fig:Misclassification} are misclassified. They are hard to judge sunny or cloudy, even for human being.
\graphicspath{ {./Figures/} }
\begin{figure}[htb]
    \centering
    \subfigure[sunny]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{err_sunny1}
		   		\label{fig:sunny}
		\end{minipage}
    	} 
    \subfigure[sunny]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{err_sunny2}
		   		\label{fig:sunny_conv1}
		\end{minipage}
    	} 
    \subfigure[sunny]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{err_sunny3}
		   		\label{fig:sunny_conv2}
		\end{minipage}
    	} 
    \subfigure[cloudy]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{err_cloudy1}
		   		\label{fig:sunny_conv3}
		\end{minipage}
    	} 
    \subfigure[cloudy]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{err_cloudy2}
		   		\label{fig:sunny_conv4}
		\end{minipage}
    	} 
    \subfigure[cloudy]{
		\begin{minipage}{0.3\textwidth}
		   		\includegraphics[width=1\textwidth]{err_cloudy3}
		   		\label{fig:sunny_conv5}
		\end{minipage}
    	} 
    \caption{Misclassified images}%
    \label{fig:Misclassification}%
\end{figure}

\section{Conclusion and Future Work}

In this project, we present an approach to do multilabel classification based on neural networks. Although the dataset is manufactured artificially, the experiment shows the strong capacity of neural networks. The project indicates that neural networks are good at approximating difficult function with adjusting topology of network. With the modification on loss function, it is effective to evaluating model which leads the error rates to converge quickly. As future work, we plan to extend approach to the more complicated datasets.