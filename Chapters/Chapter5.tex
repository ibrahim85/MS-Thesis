% Chapter 5

\chapter{Introduction} % Write in your own chapter title
\label{Chapter5}
\lhead{Chapter 5. \emph{Introduction}} % Multi-label classification


\section{Overview}

In machine learning literature, multi-label classification is a variant of the classification problem in which each instance has several labels. In general, the task of multi-label learning is to find a model that can maps inputs \textbf{x} to binary vector \textbf{y}. While it is different with multi-class classfication in terms of the later one scalars outputs in classification problem.

\graphicspath{ {./Figures/} }
\begin{figure}[!htb]
    \centering
	\includegraphics[width=0.8\textwidth]{beach.jpg}
    \caption{Example Image}%
    \label{fig:MultilableImage}%
\end{figure}

The difference between single-label classification and multi-label classification is the outputs of datasets.
In Figure \ref{fig:MultilableImage}, we can classify it as a picture of a beach in classification literature $\in \{Yes, No\}$. In multi-label literature, we can tag sea, beach, chairs, sky, cloud for the picture $\in \{beach, sea, chairs, sand\}$.

\begin{table}
\centering
\begin{tabular}{|c c c c c | c c c c|}
\hline
$X_{1}$ & $X_{2}$ &  $X_{3}$ & $X_{4}$ & $X_{5}$ & $Y_{1}$ & $Y_{2}$ & $Y_{3}$ & $Y_{4}$ \\
\hline
1 & 0.4 & 0.2 & 0 & 1 & 0 & 1 & 1 & 0 \\
0 & 0.2 & 0.6 & 1 & 1 & 1 & 0 & 1 & 0 \\
0 & 0.5 & 0.8 & 1 & 1 & 0 & 0 & 1 & 0 \\
1 & 0.1 & 0.4 & 0 & 1 & 1 & 1 & 1 & 0 \\
1 & 0.8 & 0.2 & 0 & 1 & 0 & 1 & 1 & 0 \\
\hline
0 & 0.5 & 0.3 & 1 & 1 & ? & ? & ? & ? \\
\hline
\end{tabular}
\caption{Multilabel $Y_{1},...,Y_{L} \in 2^L$}
\ref{tab:MultilableTable}
\end{table}

In general, there exists two main approaches of tackling the multi-label classification problem. One transfers a multi-label problem into a set of binary classification problems which can be handled by a set of binary classifiers. The other one applies algorithms to complete multi-label classification. 

Several problem transformation methods can be applied to multi-label classification. A baseline method, named the binary relevance method\citep{read2011classifier}, trains one binary classifier for each label independently. Depending on the results of the classifiers, the combined model predicts all labels for an unseen sample. The method divides the problem into multiple binary works which has something in common with the one-vs-all method for multi-class classification. Problem transformation benefits from scalability and flexibility because single-label classifier can be implemented to job. Support Vector Machine, Naive Bayes, $k$ Nearest Neighbor have been used in the method\citep{read2011classifier}.

There are some other transformation methods, for instance label powerset transformation. The method builds up one binary classifier for each label combination verified in the training dataset\citep{tsoumakas2006multi}. The random $k$-labelsets algorithm\citep{tsoumakas2007random} utilizes multiple label powerset classifier which is trained on a random sub dataset of the labels. A voting scheme is used to predict unseen samples via a ensemble method.

Multilayer neural network can be trained to learn a model from multi-label examples. We will generate a 3 labels dataset and use a 2 layer network to do classification.

\section{Loss Function}

To adapt a neural network handling single-label instances to multi-label instances, we need to design a specific loss function instead of squared error loss function to track the characteristics of multi-label learning and some revision of stochastic gradient descent. 

\section{Artificial Neural Networks}



\section{Weather Classification}




